# Score-based Model Configuration

# Model Configuration
model_name: "ScoreBasedDiffusion"  # Name of the model for logging
model_config:
  sigma_min: 0.01
  sigma_max: 50.0
  num_scales: 1000
  beta: 1.0
  image_size: 32
  image_channels: 3
  hidden_channels: 128
  channel_multipliers: [1, 2, 2, 2]
  num_res_blocks: 2
  attention_resolutions: [16]
  dropout: 0.1
  scale_by_sigma: true
  ema_rate: 0.999

# Training Configuration
training:
  num_epochs: 500
  batch_size: 64
  learning_rate: 2e-4
  beta1: 0.9
  beta2: 0.999
  ema_decay: 0.9999
  val_interval: 1000  # Steps between validation
  sample_interval: 5  # Epochs between sample generation
  checkpoint_interval: 10  # Epochs between checkpoints
  grad_clip: 1.0  # Gradient clipping value

# Data Configuration
data:
  dataset: "CIFAR10"  # Dataset name for logging
  image_size: 32
  channels: 3
  data_dir: "data"
  num_workers: 4

# Logging Configuration
logging:
  # Weights & Biases Configuration
  use_wandb: true
  wandb_project: "diffusion-models"
  wandb_entity: null  # Your wandb username/entity
  group: "${data.dataset}_comparison"  # Dataset-specific group for comparing models
  tags: ["score_based", "continuous", "${data.dataset}"]  # Tags for organizing runs
  notes: "Score-based continuous time model on ${data.dataset}"  # Run description
  
  # TensorBoard Configuration
  use_tensorboard: true
  tensorboard_dir: "logs"
  
  # Metric Tracking
  track_grad_norm: true  # Track gradient norms
  track_memory_usage: true  # Track GPU memory usage
  track_samples: true  # Track generated samples
  sample_visualization_frequency: 1000  # Steps between sample visualizations
  
  # Score-based Specific Tracking
  track_sigma_schedule: true  # Track noise schedule
  track_score_scaling: true  # Track score scaling factors
  track_ema_decay: true  # Track EMA rate

# Output Configuration
output:
  output_dir: "outputs/score_based"
  save_model: true
  save_optimizer: true
  save_best: true
  save_last: true
  save_frequency: 10  # Save every N epochs

# Distributed Training
distributed:
  backend: "nccl"
  find_unused_parameters: true 