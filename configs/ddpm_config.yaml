# DDPM Model Configuration

# Model Configuration
model_name: "DDPM"  # Name of the model for logging
model_config:
  time_steps: 1000
  beta_start: 0.0001
  beta_end: 0.02
  image_size: 32
  image_channels: 3
  hidden_channels: 128
  channel_multipliers: [1, 2, 2, 2]
  num_res_blocks: 2
  attention_resolutions: [16]
  dropout: 0.1
  loss_type: "mse"  # Options: mse, l1, huber, hybrid
  loss_config:
    # MSE Loss Configuration
    mse_weight: 1.0
    
    # L1 Loss Configuration
    l1_weight: 0.0
    
    # Huber Loss Configuration
    huber_weight: 0.0
    huber_delta: 1.0  # Delta parameter for Huber loss
    
    # Hybrid Loss Configuration (combination of losses)
    use_hybrid: false  # Whether to use hybrid loss
    hybrid_weights:  # Weights for different loss components
      mse: 1.0
      l1: 0.0
      huber: 0.0
    
    # Time-dependent weighting
    use_time_weighting: true  # Whether to use time-dependent loss weighting
    time_weight_type: "snr"  # Options: snr, linear, inverse
    time_weight_params:
      min_weight: 0.1
      max_weight: 1.0
    
    # Additional Loss Components
    perceptual_weight: 0.0  # Weight for perceptual loss (if used)
    adversarial_weight: 0.0  # Weight for adversarial loss (if used)

# Training Configuration
training:
  num_epochs: 500
  batch_size: 128
  learning_rate: 2e-4
  beta1: 0.9
  beta2: 0.999
  ema_decay: 0.9999
  scheduler:
    type: "cosine"  # Options: cosine, linear, step, exponential
    warmup_steps: 1000  # Number of warmup steps
    min_lr: 1e-6  # Minimum learning rate
    gamma: 0.1  # For step and exponential schedulers
    step_size: 100  # For step scheduler (in epochs)
    cycle_length: 50  # For cosine scheduler with restarts (in epochs)
    cycle_mult: 2  # Multiply cycle length by this factor after each restart
  val_interval: 1000  # Steps between validation
  sample_interval: 5  # Epochs between sample generation
  checkpoint_interval: 10  # Epochs between checkpoints

# Data Configuration
data:
  dataset: "CIFAR10"  # Dataset name for logging
  image_size: 32
  channels: 3
  data_dir: "data"
  num_workers: 4

# Logging Configuration
logging:
  # Weights & Biases Configuration
  use_wandb: true
  wandb_project: "testing"
  wandb_entity: null  # Your wandb username/entity
  group: "${data.dataset}_comparison"  # Dataset-specific group for comparing models
  tags: ["ddpm", "baseline", "${data.dataset}"]  # Tags for organizing runs
  notes: "DDPM baseline model training on ${data.dataset}"  # Run description
  
  # TensorBoard Configuration
  use_tensorboard: true
  tensorboard_dir: "logs"
  
  # Metric Tracking
  track_grad_norm: true  # Track gradient norms
  track_memory_usage: true  # Track GPU memory usage
  track_samples: true  # Track generated samples
  gradient_logging_freq: 100  # Steps between gradient logging
  sample_visualization_frequency: 1000  # Steps between sample visualizations
  
  # Detailed Metrics Configuration
  track_per_layer_metrics: true  # Track metrics for each layer
  track_parameter_histograms: true  # Track parameter distributions
  track_optimizer_stats: true  # Track optimizer statistics
  track_batch_stats: true  # Track batch statistics
  
  # Performance Monitoring
  track_time_metrics: true  # Track training time metrics
  track_gpu_stats: true  # Track detailed GPU statistics
  track_throughput: true  # Track samples/second
  
  # Diffusion Process Metrics
  track_noise_schedule: true  # Track noise schedule parameters
  track_sample_quality: true  # Track generated sample quality metrics
  track_beta_schedule: true  # Track beta schedule values
  track_timestep_embeddings: true  # Track timestep embedding statistics

# Output Configuration
output:
  output_dir: "outputs/ddpm"
  save_model: true
  save_optimizer: true
  save_best: true
  save_last: true
  save_frequency: 10  # Save every N epochs

# Distributed Training
distributed:
  backend: "nccl"
  find_unused_parameters: true 